
Welcome to my GitHub! I am **Pavan Kalyan G**, a passionate **Data Engineer** with around 3 years of experience in building scalable data pipelines, optimizing big data workflows, and ensuring data quality. 
I enjoy solving complex data challenges and leveraging tools like Spark, PySpark, and SQL to deliver efficient solutions.

---

## üîß Skills and Expertise
- *Big Data Processing*: Proficient in **PySpark**, Spark SQL, and optimizing workflows for large-scale datasets.
- *Data Engineering*: Experienced in handling nested JSONs, building data models, and working with geospatial datasets.
- *Change Data Capture (CDC)*: Implemented CDC for incremental data loads with optimized partitioning.
- *Automation*: Automated resource-based Spark job submissions using shell scripting and Kubernetes.
- *Data Warehousing*: Designed and built Data Marts using star schema for specific business use cases.
- *Performance Optimization*: Tuned Spark configurations (executor memory, partitions) and leveraged joins (broadcast, sort-merge).
- *Data Quality Assurance*: Conducted comprehensive QA analysis for data integrity before production deployment.

---

## üíº Professional Experience
- *Tata Consultancy Services (TCS)*  
  Data Engineer (3+ years)  
  - Built and optimized Enterprise Data Lake (EDL) pipelines using PySpark.  
  - Automated QA analysis for data consistency checks, including null value and count analysis.  
  - Worked on data migration projects, flattening nested JSON files with up to 5,000 schema columns.  
  - Developed geospatial data models for analyzing the expansion of footpaths in the US.

---

## üöÄ Projects
### 1. *Geospatial Data Modeling*
- Designed fact and dimension tables using SQL and Spark for mapping coordinates and polygons.
- Calculated distances between locations to analyze changes in infrastructure.

### 2. *Social Media Data Extraction*
- Processed nested JSON API responses with complex schemas to extract key insights.
- Replaced missing fields with null values to maintain schema integrity.

### 3. *Kubernetes-based Spark Job Automation*
- Developed shell scripts to check Kubernetes resource availability and trigger Spark jobs dynamically.

### 4. *Change Data Capture*
- Implemented CDC for incremental data loads using partitioning and a flag column for tracking changes.

---

## üìö Tools and Technologies
- *Languages*: Python, SQL, Shell Scripting  
- *Big Data Tools*: Apache Spark, PySpark  
- *Data Warehousing*: Star Schema, Fact/Dimension Tables, Data Marts  
- *Automation*: Jupyter Notebooks, Airflow (learning stage)  
- *DevOps*: CI/CD Pipelines, Kubernetes  
- *Visualization*: Tableau, Matplotlib  

---

## üåü Certifications & Learning
-  AWS certified AI Practitioner | Completed on AWS
-  AWS certified Data Engineer - Associate | Completed on AWS
-  Databricks certified Data Engineer Associate | Completed on Udemy

---

## ü§ù Connect with Me
- *LinkedIn*: https://www.linkedin.com/in/pavan-kalyan-493730208?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app
- *Email*: Pavankalyang789@gmail.com

Feel free to explore my repositories and reach out for collaborations or discussions!

![AI](https://github.com/user-attachments/assets/0452a39f-4ae3-4145-870c-f973df98cd1f)

![DE](https://github.com/user-attachments/assets/e68e79a1-c5de-4171-99a1-8dfd37c3dc3d)

